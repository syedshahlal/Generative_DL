{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJDZwbZMmIVfaMDhqKVmRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedshahlal/Generative_DL/blob/main/Convolution_Neural_Network_(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Convolution Neural Networks (CNN)**\n",
        "\n",
        "Convolutional Neural Networks (CNNs) applied to text for natural language processing (NLP) tasks."
      ],
      "metadata": {
        "id": "yNbDwnuG-fRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Overview**\n",
        "\n",
        "At the core of CNNs are filters (aka weights, kernels, etc.) which convolve (slide) across our input to extract relevant features. The filters are initialized randomly but learn to act as feature extractors via parameter sharing.\n"
      ],
      "metadata": {
        "id": "MFDs798t-lbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "tRKkQcuz-H6q",
        "outputId": "e240e396-95d0-400a-dbb7-6432704d62ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://madewithml.com/static/images/foundations/cnn/convolution.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(url='https://madewithml.com/static/images/foundations/cnn/convolution.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### **Objective:**\n",
        "\n",
        "  * Extract meaningful spatial substructure from encoded data.\n",
        "\n",
        "* ### **Advantages:**\n",
        "  * Small number of weights (shared)\n",
        "  * Parallelizable\n",
        "  * Detects spatial substrcutures (feature extractors)\n",
        "  * Interpretability via filters\n",
        "  * Can be used for processing in images, text, time-series, etc.\n",
        "\n",
        "* ### **Disadvantages:**\n",
        "  * Many hyperparameters (kernel size, strides, etc.) to tune.\n",
        "\n",
        "* ### **Miscellaneous:**\n",
        "  * Lot's of deep CNN architectures constantly updated for SOTA performance.\n",
        "  * Very popular feature extractor that acts as a foundation for many\n",
        "  architectures."
      ],
      "metadata": {
        "id": "WffxRQB5_kWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**\n",
        "Let's set our seed and device."
      ],
      "metadata": {
        "id": "19i95-UMASH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "OpZONG1o-Sw4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234"
      ],
      "metadata": {
        "id": "_txUBXQxAh9k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds (seed =1234):\n",
        "  \"\"\"set seeds for reproducibility.\"\"\"\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)    # multi-GPU\n"
      ],
      "metadata": {
        "id": "Vx4rO9OLA0Sj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds for reproducibilty\n",
        "set_seeds(seed=SEED)"
      ],
      "metadata": {
        "id": "3GYxBAvTBFJY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "cuda = True\n",
        "device = torch.device(\"cuda\" if (\n",
        "    torch.cuda.is_available() and cuda) else \"cpu\")\n",
        "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "if device.type == \"cuda\":\n",
        "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "print (device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCYUAeucB8sB",
        "outputId": "bbe3f378-5439-459c-9b9b-56147e24f2e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "InduL7-HCIZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}